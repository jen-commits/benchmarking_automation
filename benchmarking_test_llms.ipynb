{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/xqb6wgvd2mx6wkb_clsz33fh0000gq/T/ipykernel_605/4270990534.py:311: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet.update(col + '1', [[datetime.today().strftime('%Y%m%d')]] + [[e]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "import gspread\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import ChatPerplexity\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "global nps_advisor_evaluation_folder\n",
    "nps_advisor_evaluation_folder = '1iMwf0suglb9XnovTsVGRgE25mSCERW2p' # Output folder for AI evaluation\n",
    "\n",
    "DOC_ID = '1ifHt5uJJ4uUeaF2O_qOJXmfLR-LCF64w8ZUopWJ2xmQ' # ID for google sheet script will write to\n",
    "\n",
    "# Authentication for gsheets\n",
    "gc = gspread.oauth(\n",
    "    credentials_filename='credentials.json',\n",
    "    authorized_user_filename='token.json'\n",
    ")\n",
    "\n",
    "# Load test cases and data store\n",
    "ss = gc.open_by_key(DOC_ID)\n",
    "worksheet = ss.worksheet(\"reference_scores\")\n",
    "testcases_df = pd.DataFrame(worksheet.get_all_records())\n",
    "\n",
    "reference_scores_gsheet = gc.open_by_key(DOC_ID)\n",
    "reference_scores_sheet = reference_scores_gsheet.worksheet(\"reference_scores\")\n",
    "reference_scores_df = pd.DataFrame(reference_scores_sheet.get_all_records())\n",
    "\n",
    "# Define variables\n",
    "relevance = \"relevance\"\n",
    "depth = \"depth\"\n",
    "prioritization = \"prioritization\"\n",
    "\n",
    "relevance_reference_header = \"nps_advisor_relevance\"\n",
    "depth_reference_header = \"nps_advisor_depth\"\n",
    "prioritization_reference_header = \"nps_advisor_priority\"\n",
    "\n",
    "url = 'https://agenticworkflows.onrender.com/invoke/'\n",
    "reframe = 'true'\n",
    "guardrails = 'true'\n",
    "telco_operator = 'Circles.Life'\n",
    "payload_params = [url, reframe, guardrails, telco_operator]\n",
    "output_dir = '/output/'\n",
    "\n",
    "# Pull latest prompt from LangSmith\n",
    "relevance_prompt = hub.pull(\"benchmarking_relevance_v2\")\n",
    "depth_prompt = hub.pull(\"benchmarking_depth_v2\")\n",
    "priority_prompt = hub.pull(\"benchmarking_prioritization_v1\")\n",
    "\n",
    "prompts = {'relevance': relevance_prompt,\n",
    "           'depth': depth_prompt,\n",
    "           'prioritization': priority_prompt}\n",
    "\n",
    "# Initialize models to test\n",
    "gpt4_model = {'name': 'gpt', 'model': ChatOpenAI(\n",
    "    model=\"gpt-4o\", temperature=0.5)}\n",
    "gemini_model = {'name': 'gemini',\n",
    "                'model': ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")}\n",
    "nps_advisor = {'name': 'nps_advisor'}\n",
    "perplexity_model = {'name': 'perplexity', 'model': ChatPerplexity(temperature=0, model=\"llama-3.1-sonar-small-128k-online\")}\n",
    "reference_model = {'name': 'reference'}\n",
    "\n",
    "test_models = ['nps_advisor', 'gpt', 'gemini', 'perplexity', 'reference']\n",
    "\n",
    "# Initialize evaluator model\n",
    "eval_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "# eval_model = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_criteria_from_file(parser, eval_llm, prompt, testcases_df):\n",
    "    eval_store_data = []\n",
    "\n",
    "    # Iterate over test cases and evaluate answers from file\n",
    "    for index, row in testcases_df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        reference = row[\"reference\"]\n",
    "        answer = row[\"nps_advisor_answer\"]\n",
    "\n",
    "        # Evaluate response\n",
    "        eval_prompt_and_model = prompt | eval_llm\n",
    "        output = eval_prompt_and_model.invoke(\n",
    "            {\"question\": question, \"answer\": answer, \"reference\": reference})\n",
    "\n",
    "        # Parse the output using the parser\n",
    "        parsed_result = parser.invoke(output)\n",
    "\n",
    "        # Store results\n",
    "        eval_store_data.append(parsed_result)\n",
    "        eval_store_data[index][\"question\"] = question\n",
    "        eval_store_data[index][\"reference\"] = reference\n",
    "        eval_store_data[index][\"answer\"] = answer\n",
    "    return eval_store_data\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_llm_output(parser, eval_llm, prompt, testcases_df, answers):\n",
    "    eval_store_data = []\n",
    "\n",
    "    # Evaluate responses\n",
    "    for index, row in testcases_df.iterrows():\n",
    "        question = testcases_df.loc[index]['question']\n",
    "        reference = testcases_df.loc[index]['reference']\n",
    "\n",
    "        eval_prompt_and_model = prompt | eval_llm\n",
    "        output = eval_prompt_and_model.invoke(\n",
    "            {\"question\": question, \"answer\": answers[index], \"reference\": reference})\n",
    "\n",
    "        # Parse the output using the parser\n",
    "        parsed_result = parser.invoke(output)\n",
    "\n",
    "        # Store results\n",
    "        eval_store_data.append(parsed_result)\n",
    "        eval_store_data[index][\"question\"] = question\n",
    "        eval_store_data[index][\"reference\"] = reference\n",
    "        eval_store_data[index][\"answer\"] = answers[index][1]\n",
    "\n",
    "    return eval_store_data\n",
    "\n",
    "# Create a prompt\n",
    "def create_prompt(prompt_template, criteria):\n",
    "    # Define output schema\n",
    "    response_schemas = [\n",
    "        ResponseSchema(name=\"evaluation\", description=\"feedback on answer\"),\n",
    "        ResponseSchema(\n",
    "            name=criteria,\n",
    "            description=\"evaluation of answer, must be a percentage\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define pydanthic output parser\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    eval_prompt = PromptTemplate(\n",
    "        template=prompt_template.template,\n",
    "        input_variables=[\"question\", \"answer\", \"reference\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "    return eval_prompt, output_parser\n",
    "\n",
    "# Format score into float\n",
    "def format_score(criteria, dataframe):\n",
    "    dataframe[criteria] = dataframe[criteria].str.rstrip(\n",
    "        '%').astype('float')/100.0\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def calculate_score(relevance, depth, prioritization):\n",
    "    if math.isnan(prioritization):\n",
    "        return relevance * 0.500 + depth * 0.500\n",
    "    return relevance * 0.450 + depth * 0.450 + prioritization * 0.100\n",
    "\n",
    "\n",
    "def apply_calculate_score(dataframe):\n",
    "    return calculate_score(dataframe['relevance'], dataframe['depth'], dataframe['prioritization'])\n",
    "\n",
    "\n",
    "def get_answers(testcases_df, tested_llm, payload_params):\n",
    "    answers = []\n",
    "\n",
    "    if tested_llm['name'] == 'nps_advisor':\n",
    "        url = payload_params[0]\n",
    "        reframe = payload_params[1]\n",
    "        guardrails = payload_params[2]\n",
    "        telco_operator = payload_params[3]\n",
    "\n",
    "        # Loop through question list and get responses\n",
    "        for index, row in testcases_df.iterrows():\n",
    "            question = row[\"question\"]\n",
    "            question_reframed = urllib.parse.quote_plus(question)\n",
    "\n",
    "            # # Get answer from LLM\n",
    "            payload = \"/\".join([question_reframed, reframe,\n",
    "                                guardrails, telco_operator])\n",
    "            answer = requests.get(url + payload)\n",
    "            answers.append((index, json.loads(answer.content)['response'][0]))\n",
    "\n",
    "    else:\n",
    "        for index, row in testcases_df.iterrows():\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            # # Get answer from LLM\n",
    "            answer = tested_llm['model'].invoke(\n",
    "                [HumanMessage(content=question)]).content\n",
    "            answers.append((index, answer))\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def evaluation(prompt, criteria, testcases, test_target, answers):\n",
    "    test_models = ['nps_advisor', 'gpt', 'gemini']\n",
    "    headers = [\"question\", \"reference\", \"answer\", \"evaluation\"]\n",
    "    headers.append(criteria)\n",
    "\n",
    "    prompt, parser = create_prompt(prompt, criteria)\n",
    "    if criteria == \"prioritization\":  # Select only rows with prioritization flag\n",
    "        testcases = testcases.loc[testcases['prioritization_flag'] == 1]\n",
    "\n",
    "    if test_target['name'] == 'reference':\n",
    "        store_data = evaluate_criteria_from_file(\n",
    "            parser, eval_model, prompt, testcases)\n",
    "        df = pd.DataFrame(store_data, columns=headers)\n",
    "        df = format_score(criteria, df)\n",
    "        \n",
    "        Path(os.getcwd() + output_dir + test_target['name'] + \"/\").mkdir(parents=True, exist_ok=True)\n",
    "        df.to_excel(os.getcwd() + \n",
    "                    output_dir + \n",
    "                    test_target['name'] + \"/\" +\n",
    "                    datetime.today().strftime('%Y%m%d') + \"_\" + criteria + \"_score.xlsx\")\n",
    "    else:\n",
    "        store_data = evaluate_llm_output(\n",
    "            parser, eval_model, prompt, testcases, answers)\n",
    "        \n",
    "        df = pd.DataFrame(store_data, columns=headers)\n",
    "        df = format_score(criteria, df)\n",
    "\n",
    "        sh = gc.create(datetime.today().strftime('%Y%m%d') + \"_\" +\n",
    "                    criteria + \"_scoring.xlsx\",\n",
    "                    folder_id=nps_advisor_evaluation_folder)\n",
    "        worksheet = sh.get_worksheet(0)\n",
    "        worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def colnum_string(n):\n",
    "    string = \"\"\n",
    "    while n > 0:\n",
    "        n, remainder = divmod(n - 1, 26)\n",
    "        string = chr(65 + remainder) + string\n",
    "    return string\n",
    "\n",
    "\n",
    "# Main exec\n",
    "def execute_benchmarks(checks, prompts, testcases, test_target, answers):\n",
    "    for check in checks:\n",
    "        match check:\n",
    "            case \"relevance\":\n",
    "                criteria = 'relevance'\n",
    "                prompt = prompts[criteria]\n",
    "                rel_df = evaluation(\n",
    "                    prompt, criteria, testcases, test_target, answers)\n",
    "            case \"depth\":\n",
    "                criteria = 'depth'\n",
    "                prompt = prompts[criteria]\n",
    "                dep_df = evaluation(\n",
    "                    prompt, criteria, testcases, test_target, answers)\n",
    "            case \"prioritization\":\n",
    "                criteria = 'prioritization'\n",
    "                prompt = prompts[criteria]\n",
    "                pri_df = evaluation(\n",
    "                    prompt, criteria, testcases, test_target, answers)\n",
    "\n",
    "    # Calculate weighted score\n",
    "    if len(checks) == 3:\n",
    "        dep_join = dep_df.columns.difference(rel_df.columns)\n",
    "        pri_join = pri_df.columns.difference(rel_df.columns)\n",
    "        output_df = pd.merge(rel_df, dep_df[dep_join], left_index=True,\n",
    "                             right_index=True, how=\"outer\")\n",
    "        output_df = pd.merge(output_df, pri_df[pri_join], left_index=True,\n",
    "                             right_index=True, how=\"outer\")\n",
    "        output_df[\"weighted_score\"] = output_df.apply(\n",
    "            apply_calculate_score, axis=1)\n",
    "\n",
    "        return output_df\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "##### Execution Variables ######\n",
    "# Target to test: gpt4_model, gemini_model, nps_advisor, perplexity_model, reference_model\n",
    "test_target = nps_advisor\n",
    "\n",
    "# Switches for debugging\n",
    "checks = ['relevance', 'depth', 'prioritization']\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    if test_target['name'] == 'reference':\n",
    "        answers = ''\n",
    "    else:\n",
    "        answers = get_answers(testcases_df, test_target, payload_params)\n",
    "\n",
    "    weighted_score_df = execute_benchmarks(checks,\n",
    "                                           prompts,\n",
    "                                           testcases_df,\n",
    "                                           test_target,\n",
    "                                           answers)\n",
    "\n",
    "    if test_target['name'] == 'nps_advisor':\n",
    "        weighted_today = weighted_score_df.loc[:, 'weighted_score'].astype(str)\n",
    "        ss = gc.open_by_key(DOC_ID)\n",
    "        worksheet = ss.worksheet(\"benchmarking\")\n",
    "        values = worksheet.get_all_values()\n",
    "        col = colnum_string(max([len(r) for r in values]) + 1)\n",
    "        worksheet.update(col + '1', [[datetime.today().strftime('%Y%m%d')]] + [[e]\n",
    "                                                                               for e in weighted_today.tolist()], value_input_option='USER_ENTERED')\n",
    "    \n",
    "    if test_target['name'] == 'reference':\n",
    "        Path(os.getcwd() + output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        weighted_score_df.to_excel(os.getcwd() + output_dir +\n",
    "                                   datetime.today().strftime('%Y%m%d') + \"_\" +\n",
    "                                   test_target['name'] +\n",
    "                                   \"_weighted_score.xlsx\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_score(relevance, depth, prioritization):\n",
    "    if math.isnan(prioritization):\n",
    "        return relevance * 0.500 + depth * 0.500\n",
    "    return relevance * 0.450 + depth * 0.450 + prioritization * 0.100\n",
    "\n",
    "\n",
    "def apply_calculate_score(dataframe):\n",
    "    return calculate_score(dataframe['relevance'], dataframe['depth'], dataframe['prioritization'])\n",
    "\n",
    "\n",
    "dep_join = dep_df.columns.difference(rel_df.columns)\n",
    "pri_join = pri_df.columns.difference(rel_df.columns)\n",
    "output_df = pd.merge(rel_df, dep_df[dep_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "output_df = pd.merge(output_df, pri_df[pri_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "\n",
    "# display(output_df)\n",
    "output_df[\"weighted_score\"] = output_df.apply(apply_calculate_score, axis=1)\n",
    "\n",
    "Path(os.getcwd() + output_dir).mkdir(parents=True, exist_ok=True)\n",
    "output_df.to_excel(os.getcwd() + output_dir +\n",
    "                   test_target['name'] + \"_weighted_score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(weighted_score_df.loc[:, 'weighted_score'])\n",
    "weighted_today = weighted_score_df.loc[:, 'weighted_score']\n",
    "weighted_today.values.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1Yb83yEk36ptZa0dzg_i4jAr8koe43MXNkJVd66431To',\n",
       " 'updatedRange': 'Sheet1!A1:Q16',\n",
       " 'updatedRows': 16,\n",
       " 'updatedColumns': 17,\n",
       " 'updatedCells': 272}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "\n",
    "# weighted_today = weighted_score_df.loc[:, 'weighted_score'].astype(str)\n",
    "# weighted_today.describe\n",
    "\n",
    "\n",
    "def colnum_string(n):\n",
    "    string = \"\"\n",
    "    while n > 0:\n",
    "        n, remainder = divmod(n - 1, 26)\n",
    "        string = chr(65 + remainder) + string\n",
    "    return string\n",
    "\n",
    "\n",
    "gc = gspread.oauth(\n",
    "    credentials_filename='/Users/jenyang.niam/Projects/benchmarking_automation/credentials.json',\n",
    "    authorized_user_filename='/Users/jenyang.niam/Projects/benchmarking_automation/token.json'\n",
    ")\n",
    "# gc = gspread.oauth()\n",
    "\n",
    "# gc = gspread.service_account(filename='credentials.json')\n",
    "DOC_ID = '1ifHt5uJJ4uUeaF2O_qOJXmfLR-LCF64w8ZUopWJ2xmQ'\n",
    "SHEET_NAME = 'Sheet1'\n",
    "DOC_URL = 'https://docs.google.com/spreadsheets/d/' + \\\n",
    "    DOC_ID + '/gviz/tq?tqx=out:csv&sheet=' + SHEET_NAME\n",
    "\n",
    "ss = gc.open_by_key(DOC_ID)\n",
    "worksheet = ss.worksheet(\"reference_scores\")\n",
    "\n",
    "\n",
    "testcases_df = pd.DataFrame(worksheet.get_all_records())\n",
    "\n",
    "sh = gc.create('A new spreadsheet',\n",
    "               folder_id='1NirPog7MubSuwe4Jv4dspU6EzHV9XTJL')\n",
    "worksheet2 = sh.get_worksheet(0)\n",
    "worksheet2.update([testcases_df.columns.values.tolist()] +\n",
    "                  testcases_df.values.tolist())\n",
    "# for index, row in testcases_df.iterrows():\n",
    "#     print(index, row)\n",
    "\n",
    "# testcases_df = pd.read_csv(\"testcases_v1.csv\")\n",
    "# print(questions)\n",
    "# print(reference)\n",
    "# rang = 'A' + str(len(values_list)+1)\n",
    "# worksheet.update(rang, [[\"20240911\"]] + [[e] for e in weighted_today.tolist()])\n",
    "\n",
    "\n",
    "# values = worksheet.get_all_values()\n",
    "# col = colnum_string(max([len(r) for r in values]) + 1)\n",
    "# worksheet.update(col + '1', [[\"20240911\"]] + [[e]\n",
    "#                  for e in weighted_today.tolist()], value_input_option='USER_ENTERED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

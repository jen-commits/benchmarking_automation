{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'rel_df' (DataFrame)\n",
      "Stored 'dep_df' (DataFrame)\n",
      "Stored 'pri_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_criteria_from_file(parser, eval_llm, prompt, testcases_df):\n",
    "    eval_store_data = []\n",
    "\n",
    "    # Iterate over test cases and evaluate answers from file\n",
    "    for index, row in testcases_df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        reference = row[\"reference\"]\n",
    "        answer = row[\"nps_advisor_answer\"]\n",
    "\n",
    "        # Evaluate response\n",
    "        eval_prompt_and_model = prompt | eval_llm\n",
    "        output = eval_prompt_and_model.invoke(\n",
    "            {\"question\": question, \"answer\": answer, \"reference\": reference})\n",
    "\n",
    "        # Parse the output using the parser\n",
    "        parsed_result = parser.invoke(output)\n",
    "\n",
    "        # Store results\n",
    "        eval_store_data.append(parsed_result)\n",
    "        eval_store_data[index][\"question\"] = question\n",
    "        eval_store_data[index][\"reference\"] = reference\n",
    "        eval_store_data[index][\"answer\"] = answer\n",
    "    return eval_store_data\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_llm_output(parser, eval_llm, prompt, testcases_df, answers):\n",
    "    eval_store_data = []\n",
    " \n",
    "    # Evaluate responses\n",
    "    for index, row in testcases_df.iterrows():\n",
    "        question = testcases_df.loc[index]['question']\n",
    "        reference = testcases_df.loc[index]['reference']\n",
    "        \n",
    "        eval_prompt_and_model = prompt | eval_llm\n",
    "        output = eval_prompt_and_model.invoke(\n",
    "            {\"question\": question, \"answer\": answers[index], \"reference\": reference})\n",
    "\n",
    "        # Parse the output using the parser\n",
    "        parsed_result = parser.invoke(output)\n",
    "\n",
    "        # Store results\n",
    "        eval_store_data.append(parsed_result)\n",
    "        eval_store_data[index][\"question\"] = question\n",
    "        eval_store_data[index][\"reference\"] = reference\n",
    "        eval_store_data[index][\"answer\"] = answers[index][1]\n",
    "        \n",
    "    return eval_store_data\n",
    "\n",
    "# Create a prompt\n",
    "def create_prompt(prompt_template, criteria):\n",
    "    # Define output schema\n",
    "    response_schemas = [\n",
    "        ResponseSchema(name=\"evaluation\", description=\"feedback on answer\"),\n",
    "        ResponseSchema(\n",
    "            name=criteria,\n",
    "            description=\"evaluation of answer, must be a percentage\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define pydanthic output parser\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    eval_prompt = PromptTemplate(\n",
    "        template=prompt_template.template,\n",
    "        input_variables=[\"question\", \"answer\", \"reference\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "    return eval_prompt, output_parser\n",
    "\n",
    "# Format score into float\n",
    "def format_score(criteria, dataframe):\n",
    "    dataframe[criteria] = dataframe[criteria].str.rstrip(\n",
    "        '%').astype('float')/100.0\n",
    "    return dataframe\n",
    "\n",
    "def calculate_score(relevance, depth, prioritization):\n",
    "    if math.isnan(prioritization):\n",
    "        return relevance * 0.500 + depth * 0.500\n",
    "    return relevance * 0.450 + depth * 0.450 + prioritization * 0.100\n",
    "\n",
    "def apply_calculate_score(dataframe):\n",
    "    return calculate_score(dataframe['relevance'], dataframe['depth'], dataframe['prioritization'])\n",
    "\n",
    "def get_answers(testcases_df, tested_llm, payload_params):\n",
    "    answers = []    \n",
    "    \n",
    "    if tested_llm['name'] == 'nps_advisor':\n",
    "        url = payload_params[0]\n",
    "        reframe = payload_params[1]\n",
    "        guardrails = payload_params[2]\n",
    "        telco_operator = payload_params[3]\n",
    "        \n",
    "        # Loop through question list and get responses\n",
    "        for index, row in testcases_df.iterrows():\n",
    "            question = row[\"question\"]\n",
    "            question_reframed = urllib.parse.quote_plus(question)\n",
    "\n",
    "            # # Get answer from LLM\n",
    "            payload = \"/\".join([question_reframed, reframe,\n",
    "                            guardrails, telco_operator])\n",
    "            answer = requests.get(url + payload)\n",
    "            answers.append((index, json.loads(answer.content)['response'][0]))\n",
    "            \n",
    "    else:\n",
    "        for index, row in testcases_df.iterrows():\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            # # Get answer from LLM\n",
    "            answer = tested_llm['model'].invoke([HumanMessage(content=question)]).content\n",
    "            answers.append((index, answer))\n",
    "    \n",
    "    return answers\n",
    "  \n",
    "# Main exec\n",
    "def execute_benchmarks(prompt, testcases, criteria, \n",
    "                       headers, test_target, answers):\n",
    "    test_models = ['nps_advisor', 'gpt', 'gemini']\n",
    "    \n",
    "    prompt, parser = create_prompt(prompt, criteria)\n",
    "    if criteria == \"prioritization\":  # Select only rows with prioritization flag\n",
    "        testcases = testcases.loc[testcases['prioritization_flag'] == 1]\n",
    "\n",
    "    if test_target['name'] in test_models:\n",
    "        store_data = evaluate_llm_output(\n",
    "            parser, eval_model, prompt, testcases, answers)\n",
    "    else:\n",
    "        store_data = evaluate_criteria_from_file(\n",
    "            parser, eval_model, prompt, testcases)\n",
    "    df = pd.DataFrame(store_data, columns=headers)\n",
    "    df = format_score(criteria, df)\n",
    "\n",
    "    # Save output\n",
    "    \n",
    "    Path(os.getcwd() + output_dir + test_target['name'] + \"/\").mkdir(parents=True, exist_ok=True)\n",
    "    df.to_excel(os.getcwd() + output_dir + test_target['name'] + \"/\" + criteria + \"_scoring.xlsx\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load test cases and data store\n",
    "testcases_df = pd.read_csv(\"testcases_v1.csv\")\n",
    "reference_scores_df = pd.read_csv(\"reference_scoring_v1.csv\")\n",
    "\n",
    "# Define variables\n",
    "relevance = \"relevance\"\n",
    "depth = \"depth\"\n",
    "prioritization = \"prioritization\"\n",
    "\n",
    "relevance_reference_header = \"nps_advisor_relevance\"\n",
    "depth_reference_header = \"nps_advisor_depth\"\n",
    "prioritization_reference_header = \"nps_advisor_priority\"\n",
    "\n",
    "relevance_answer_headers = [\"question\",\n",
    "                            \"reference\", \"answer\", \"evaluation\", \"relevance\"]\n",
    "depth_answer_headers = [\"question\", \"reference\",\n",
    "                        \"answer\", \"evaluation\", \"depth\"]\n",
    "priority_answer_headers = [\"question\", \"reference\",\n",
    "                           \"answer\", \"evaluation\", \"prioritization\"]\n",
    "\n",
    "url = 'https://agenticworkflows.onrender.com/invoke/'\n",
    "reframe = 'true'\n",
    "guardrails = 'true'\n",
    "telco_operator = 'Circles.Life'\n",
    "payload_params = [url, reframe, guardrails, telco_operator]\n",
    "\n",
    "output_dir = '/output/'\n",
    "\n",
    "# Pull latest prompt from LangSmith\n",
    "relevance_prompt = hub.pull(\"benchmarking_relevance_v1\")\n",
    "depth_prompt = hub.pull(\"benchmarking_depth_v1\")\n",
    "priority_prompt = hub.pull(\"benchmarking_prioritization_v1\")\n",
    "\n",
    "# Initialize models to test\n",
    "gpt4_model = {'name': 'gpt', 'model':ChatOpenAI(model=\"gpt-4o\", temperature=0.5)}\n",
    "gemini_model = {'name': 'gemini', 'model': ChatGoogleGenerativeAI(model=\"gemini-pro\")}\n",
    "nps_advisor = {'name': 'nps_advisor'}\n",
    "\n",
    "test_models = ['nps_advisor', 'gpt', 'gemini']\n",
    "\n",
    "# Initialize evaluator model\n",
    "eval_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "# Target\n",
    "test_target = nps_advisor\n",
    "\n",
    "# Switches\n",
    "test_relevence = True\n",
    "test_depth = True\n",
    "test_priority = True\n",
    "\n",
    "# Run relevance benchmark\n",
    "if test_target['name'] in test_models:\n",
    "    answers = get_answers(testcases_df, test_target, payload_params)\n",
    "else:\n",
    "    answers = ''\n",
    "                      \n",
    "if test_relevence:\n",
    "    rel_df = execute_benchmarks(relevance_prompt, testcases_df,\n",
    "                                relevance, relevance_answer_headers, \n",
    "                                test_target, answers)\n",
    "    %store rel_df\n",
    "\n",
    "if test_depth:\n",
    "    dep_df = execute_benchmarks(depth_prompt, testcases_df,\n",
    "                                depth, depth_answer_headers, \n",
    "                                test_target, answers)\n",
    "    %store dep_df\n",
    "\n",
    "if test_priority:\n",
    "    pri_df = execute_benchmarks(priority_prompt, testcases_df,\n",
    "                                prioritization, priority_answer_headers, \n",
    "                                test_target, answers)\n",
    "    %store pri_df\n",
    "    \n",
    "# Calculate weighted score\n",
    "dep_join = dep_df.columns.difference(rel_df.columns)\n",
    "pri_join = pri_df.columns.difference(rel_df.columns)\n",
    "output_df = pd.merge(rel_df, dep_df[dep_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "output_df = pd.merge(output_df, pri_df[pri_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "\n",
    "output_df[\"weighted_score\"] = output_df.apply(apply_calculate_score, axis=1)\n",
    "\n",
    "Path(os.getcwd() + output_dir).mkdir(parents=True, exist_ok=True)\n",
    "output_df.to_excel(os.getcwd() + output_dir +\n",
    "                   test_target['name'] + \"_weighted_score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_score(relevance, depth, prioritization):\n",
    "    if math.isnan(prioritization):\n",
    "        return relevance * 0.500 + depth * 0.500\n",
    "    return relevance * 0.450 + depth * 0.450 + prioritization * 0.100\n",
    "\n",
    "\n",
    "def apply_calculate_score(dataframe):\n",
    "    return calculate_score(dataframe['relevance'], dataframe['depth'], dataframe['prioritization'])\n",
    "\n",
    "\n",
    "dep_join = dep_df.columns.difference(rel_df.columns)\n",
    "pri_join = pri_df.columns.difference(rel_df.columns)\n",
    "output_df = pd.merge(rel_df, dep_df[dep_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "output_df = pd.merge(output_df, pri_df[pri_join], left_index=True,\n",
    "                     right_index=True, how=\"outer\")\n",
    "\n",
    "# display(output_df)\n",
    "output_df[\"weighted_score\"] = output_df.apply(apply_calculate_score, axis=1)\n",
    "\n",
    "Path(os.getcwd() + output_dir).mkdir(parents=True, exist_ok=True)\n",
    "output_df.to_excel(os.getcwd() + output_dir +\n",
    "                   test_target['name'] + \"_weighted_score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                             1\n",
       "question               Summarize the NPS for this year (2024) for Cir...\n",
       "prioritization_flag                                                    1\n",
       "reference              Average NPS of -17 in 2024. \\n\\nJanuary 2024: ...\n",
       "nps_advisor_answer     As of 2024, Circles.Life has experienced fluct...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "testcases_df.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
